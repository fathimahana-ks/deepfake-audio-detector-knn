# -*- coding: utf-8 -*-
"""Synthetic Audio Identification - KNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qIdZ-4CAoCQtwL97MBI8RO6FQJwNtdPL

# **SYNTHETIC AUDIO IDENTIFICATION USING DEEPFAKE DETECTION**

---

#### **Module 0: Visualization of Fake & Real Audio and Features**
"""

import IPython
real_audio = '/content/deep-voice-dataset/demonstration/linus-original-DEMO.mp3'
fake_audio = '/content/deep-voice-dataset/demonstration/linus-to-musk-DEMO.mp3'

print('Real Audio:')
IPython.display.Audio(real_audio)

print('Fake Audio:')
IPython.display.Audio(fake_audio)

real_ad, real_sr = librosa.load(real_audio)
plt.figure(figsize=(12, 4))
plt.plot(real_ad)
plt.title("Waveform of Real Audio Data")
plt.savefig('fig1.png', dpi=300, bbox_inches='tight')
plt.show()

fake_ad, fake_sr = librosa.load(fake_audio)
plt.figure(figsize=(12, 4))
plt.plot(fake_ad)
plt.title("Waveform of Fake Audio Data")
plt.savefig('fig4.png', dpi=300, bbox_inches='tight')
plt.show()

data_path = "/content/deep-voice-dataset/dataset/DATASET-balanced.csv"
df = pd.read_csv(data_path)
df.head()

"""The dataset contains processed features extracted from audio files.

#### **Module 1: Data Loading & Preprocessing**
"""

!pip install librosa #for audio analysis and processing
!pip install pydub #audio file manipulation and conversion

#Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

#Define paths
root_dir = '/content/drive/MyDrive/deep-voice-dataset'
demo = '/content/drive/MyDrive/deep-voice-dataset/DEMONSTRATION'
dataset = '/content/drive/MyDrive/deep-voice-dataset/DATASET'

#Ensure the data directories exist
os.makedirs('deep-voice-dataset/demonstration', exist_ok=True)
os.makedirs('deep-voice-dataset/dataset', exist_ok=True)

#Copy dataset
def copy_dataset(src_dir, dest_dir):
    if os.path.exists(dest_dir):
        shutil.rmtree(dest_dir)
    shutil.copytree(src_dir, dest_dir)

# Copy demo dataset
copy_dataset(demo, 'deep-voice-dataset/demonstration')

# Copy original dataset
copy_dataset(dataset, 'deep-voice-dataset/dataset')

# List dataset files
for dirname, _, filenames in os.walk('/content/deep-voice-dataset'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""#### **Module 2: Audio segmentation & Data Augmentation**"""

import os
import librosa
from pydub import AudioSegment
import random

#Define dataset paths
dataset_path = "/content/deep-voice-dataset/dataset/AUDIO"
augmented_dataset_path = "/content/SPLITTED"

## Create directories for augmented data
os.makedirs(augmented_dataset_path, exist_ok=True)
os.makedirs(os.path.join(augmented_dataset_path, "FAKE"), exist_ok=True)
os.makedirs(os.path.join(augmented_dataset_path, "REAL"), exist_ok=True)

def split_audio_file(file_path, output_dir, segment_length=10):
    audio = AudioSegment.from_wav(file_path) #Split an audio file into segments of a given length (in seconds).
    duration = audio.duration_seconds# Convert to milliseconds
    segment_length_ms = segment_length * 1000

    segments = []
    for i in range(0, len(audio), segment_length_ms):
        segment = audio[i:i + segment_length_ms]
        segment_file_name = f"{os.path.splitext(os.path.basename(file_path))[0]}_part{i // segment_length_ms}.wav"
        segment_path = os.path.join(output_dir, segment_file_name)
        segment.export(segment_path, format="wav")
        segments.append(segment_path)
    return segments

#Split real audio files into 10-second segments
real_samples_path = os.path.join(dataset_path, "REAL")
for file_name in os.listdir(real_samples_path):
    file_path = os.path.join(real_samples_path, file_name)
    output_dir = os.path.join(augmented_dataset_path, "REAL")
    split_audio_file(file_path, output_dir)

# Split fake audio files into 10-second segments
fake_samples_path = os.path.join(dataset_path, "FAKE")
fake_segments = []
for file_name in os.listdir(fake_samples_path):
    file_path = os.path.join(fake_samples_path, file_name)
    output_dir = os.path.join(augmented_dataset_path, "FAKE")
    fake_segments.extend(split_audio_file(file_path, output_dir))

# Randomly select fake audio segments to match the number of real samples
selected_fake_segments = random.sample(fake_segments, len(os.listdir("/content/SPLITTED/REAL")))

# Create directory for selected fake samples augmentation
selected_fake_path = os.path.join(augmented_dataset_path, "SELECTED_FAKE")
os.makedirs(selected_fake_path, exist_ok=True)

# Move selected fake audio files to the new directory
for segment in selected_fake_segments:
    segment_file_name = os.path.basename(segment)
    os.rename(segment, os.path.join(selected_fake_path, segment_file_name))

"""#### **Module 3: Feature Extraction (Mel-Frequency Cepstral Coefficients)**"""

import os
import numpy as np
import librosa
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# extract MFCC features
def extract_features(file_path, n_mfcc=13):
    y, sr = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    return np.mean(mfccs.T, axis=0)

fake_samples_path = "/content/SPLITTED/SELECTED_FAKE"
real_samples_path = "/content/SPLITTED/REAL"

features = []
labels = []

# Process fake audio samples
for file_name in os.listdir(fake_samples_path):
    file_path = os.path.join(fake_samples_path, file_name)
    if file_path.endswith('.wav'):
        features.append(extract_features(file_path))
        labels.append(0)  # Label 0 for fake

# Process real audio samples
for file_name in os.listdir(real_samples_path):
    file_path = os.path.join(real_samples_path, file_name)
    if file_path.endswith('.wav'):
        features.append(extract_features(file_path))
        labels.append(1)  # Label 1 for real

"""#### **Module 4: Data Splitting (Train & Test)**"""

# Split data into training and testing sets
X = np.array(features)
y = np.array(labels)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""#### **Module 5: Model Training (KNN Classifier)**"""

from sklearn.neighbors import KNeighborsClassifier

# Call the KNN model
knn = KNeighborsClassifier()

# training model KNN
knn.fit(X_train, y_train)

# Make predictions using the trained model
y_pred = knn.predict(X_test)

# evalauate model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print evaluation metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")

"""#### **Module 6: Model Evaluation**"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# print confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Fake", "Real"], yticklabels=["Fake", "Real"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""#### **Module 7: Model Saving**"""

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

import joblib

# File path to save the model
model_path = 'knn_model.pkl'

# Save the model to disk
joblib.dump(knn, model_path)

"""#### **Module 8: Model Deployment**

"""

#using the model
def extract_features(file_path, n_mfcc=13):
    y, sr = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    return np.mean(mfccs.T, axis=0)
new_audio='/content/deep-voice-dataset/dataset/AUDIO/REAL/musk-original.wav'
feature=extract_features(new_audio)
print(feature)

model_path='/content/knn_model.pkl'
load_model=joblib.load(model_path)
feature = np.array(feature).reshape(1, -1)

prediction=load_model.predict(feature)
print("Predicted value:",prediction[0])