# ðŸŽ§ Deepfake Audio Detector - KNN

A K-Nearest Neighbors (KNN)-based system to classify synthetic (deepfake) and real audio using Mel-Frequency Cepstral Coefficients (MFCCs).

---

## ðŸ” Overview

This project identifies whether an audio sample is real or AI-generated using:
- **K-Nearest Neighbors (KNN)** classifier
- **MFCC feature extraction**
- **Dataset of real and fake voices**
- **Audio preprocessing & segmentation**

---

## ðŸ§  Key Features

- ðŸ“Š **KNN Classification** with scikit-learn  
- ðŸŽµ **MFCC Extraction** using `librosa`  
- ðŸ”ˆ **10s Audio Segmentation** for dataset balancing  
- ðŸ“ Train/Test split with evaluation metrics  
- ðŸ“¦ Model saved using `joblib` for reuse  

---

## ðŸ“ Project Structure
deepfake-audio-detector-knn/
â”‚
â”œâ”€â”€ data/ # Processed dataset and demo audio
â”‚ â”œâ”€â”€ REAL/
â”‚ â””â”€â”€ FAKE/
â”‚
â”œâ”€â”€ notebooks/ # Colab-compatible Python code
â”‚ â””â”€â”€ deepfake_knn.ipynb
â”‚
â”œâ”€â”€ models/ # Saved model
â”‚ â””â”€â”€ knn_model.pkl
â”‚
â”œâ”€â”€ results/ # Evaluation results and visualizations
â”‚ â””â”€â”€ confusion_matrix.png
â”‚
â””â”€â”€ README.md # This file


---

## ðŸ› ï¸ Technologies Used

- Python
- Librosa
- Pydub
- Scikit-learn
- Matplotlib & Seaborn

---

## ðŸ“Š Evaluation Metrics

| Metric     | Value (Example) |
|------------|-----------------|
| Accuracy   | 92%             |
| Precision  | 90%             |
| Recall     | 93%             |
| F1-Score   | 91%             |

> ðŸ“Œ Confusion matrix and classification report are included in the `results/` folder.

---

## ðŸš€ How to Run

1. Clone this repo  
2. Install required packages:
   ```bash
   pip install -r requirements.txt

3. Run the Jupyter notebook or convert it to a Python script:

   ```bash
   jupyter notebook notebooks/deepfake_knn.ipynb

## ðŸ“¦ Model Deployment

Use the saved  model to make predictions on new audio features:

   ```bash
   import joblib
   
   model = joblib.load('models/knn_model.pkl')
   prediction = model.predict([your_features])
   print("Prediction:", prediction[0])
   ```
---

## ðŸ“Œ Notes

- The dataset requires balanced classes (equal number of fake and real audio clips).  
- Audio clips are segmented into fixed 10-second intervals.  
- Features are based on MFCCs for effective audio representation.  
- This project focuses on KNN; additional models (CNN, Naive Bayes) are in separate branches/repositories.  
- Make sure dependencies and data paths are properly configured for smooth execution.

---

## ðŸ“¬ Contact

**Fathima Hana**  
ðŸ“§ [fathimahanaks@gmail.com](mailto:fathimahanaks@gmail.com)  
ðŸ”— [LinkedIn](https://www.linkedin.com/in/fathimahana/)  <!-- Replace with your actual LinkedIn URL -->

Feel free to reach out for collaborations or questions!
